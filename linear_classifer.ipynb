{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting linear_classifer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile linear_classifer.py\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def softmax(predictions):\n",
    "    '''\n",
    "    Computes probabilities from scores\n",
    "    Arguments:\n",
    "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
    "        classifier output\n",
    "    Returns:\n",
    "      probs, np array of the same shape as predictions - \n",
    "        probability for every class, 0..1\n",
    "    '''\n",
    "    prediction = predictions.copy()\n",
    "    new_exp = np.vectorize(lambda x: math.exp(x))\n",
    "    if len(predictions.shape)==1:\n",
    "        pred = prediction-np.max(prediction)\n",
    "        exp_prob = new_exp(pred)\n",
    "        probs = np.array(list(map(lambda x: x/np.sum(exp_prob),exp_prob)))\n",
    "    else:\n",
    "        pred = list(map(lambda x: x-np.max(x), prediction))\n",
    "        exp_prob = new_exp(pred)\n",
    "        probs = np.array(list(map(lambda x: x/np.sum(x),exp_prob)))\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def cross_entropy_loss(probs, target_index):\n",
    "    '''\n",
    "    Computes cross-entropy loss\n",
    "    Arguments:\n",
    "      probs, np array, shape is either (N) or (batch_size, N) -\n",
    "        probabilities for every class\n",
    "      target_index: np array of int, shape is (1) or (batch_size) -\n",
    "        index of the true class for given sample(s)\n",
    "    Returns:\n",
    "      loss: single value\n",
    "    '''\n",
    "    new_loss = np.vectorize(lambda x: -math.log(x))\n",
    "    if len(probs.shape)==1:\n",
    "        probs_target = probs[target_index]\n",
    "        size_target = 1\n",
    "    else:\n",
    "        batch_size = np.arange(target_index.shape[0])\n",
    "        probs_target = probs[batch_size,target_index.flatten()]\n",
    "        size_target = target_index.shape[0]\n",
    "    loss = np.sum(new_loss(probs_target))/size_target\n",
    "    return loss\n",
    "\n",
    "\n",
    "def softmax_with_cross_entropy(predictions, target_index):\n",
    "    '''\n",
    "    Computes softmax and cross-entropy loss for model predictions,\n",
    "    including the gradient\n",
    "    Arguments:\n",
    "      predictions, np array, shape is either (N) or (batch_size, N) -\n",
    "        classifier output\n",
    "      target_index: np array of int, shape is (1) or (batch_size) -\n",
    "        index of the true class for given sample(s)\n",
    "    Returns:\n",
    "      loss, single value - cross-entropy loss\n",
    "      dprediction, np array same shape as predictions - gradient of predictions by loss value\n",
    "    '''\n",
    "    prediction = predictions.copy()\n",
    "    probs = softmax(prediction) \n",
    "    loss = cross_entropy_loss(probs, target_index)\n",
    "    dprediction = probs\n",
    "    if len(predictions.shape)==1:\n",
    "        dprediction[target_index] -= 1\n",
    "    else:\n",
    "        batch_size = np.arange(target_index.shape[0])\n",
    "        dprediction[batch_size,target_index.flatten()] -= 1\n",
    "        dprediction = dprediction/target_index.shape[0]\n",
    "\n",
    "    return loss, dprediction\n",
    "\n",
    "\n",
    "def l2_regularization(W, reg_strength):\n",
    "    '''\n",
    "    Computes L2 regularization loss on weights and its gradient\n",
    "    Arguments:\n",
    "      W, np array - weights\n",
    "      reg_strength - float value\n",
    "    Returns:\n",
    "      loss, single value - l2 regularization loss\n",
    "      gradient, np.array same shape as W - gradient of weight by l2 loss\n",
    "    '''\n",
    "\n",
    "    loss = reg_strength*np.sum(np.dot(np.transpose(W),W))\n",
    "    batch_size = np.arange(W.shape[1])\n",
    "    grad = np.array((list(map(lambda x: np.sum(W,axis=1), batch_size))))\n",
    "    grad = 2*reg_strength*np.transpose(grad)\n",
    "\n",
    "    return loss, grad\n",
    "    \n",
    "\n",
    "def linear_softmax(X, W, target_index):\n",
    "    '''\n",
    "    Performs linear classification and returns loss and gradient over W\n",
    "    Arguments:\n",
    "      X, np array, shape (num_batch, num_features) - batch of images\n",
    "      W, np array, shape (num_features, classes) - weights\n",
    "      target_index, np array, shape (num_batch) - index of target classes\n",
    "    Returns:\n",
    "      loss, single value - cross-entropy loss\n",
    "      gradient, np.array same shape as W - gradient of weight by loss\n",
    "    '''\n",
    "    predictions = np.dot(X, W)\n",
    "\n",
    "    prediction = predictions.copy()\n",
    "    probs = softmax(prediction) \n",
    "    loss = cross_entropy_loss(probs, target_index)\n",
    "    dW = np.dot(np.transpose(X),probs)\n",
    "    \n",
    "    p = np.zeros_like(probs)\n",
    "    batch_size = np.arange(target_index.shape[0])\n",
    "    p[batch_size,target_index.flatten()] = 1\n",
    "    dW -= np.transpose(np.dot(np.transpose(p),X))\n",
    "    dW = dW/target_index.shape[0]\n",
    "    \n",
    "    return loss, dW\n",
    "\n",
    "\n",
    "class LinearSoftmaxClassifier():\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y, batch_size=100, learning_rate=1e-7, reg=1e-5,\n",
    "            epochs=1):\n",
    "        '''\n",
    "        Trains linear classifier\n",
    "        \n",
    "        Arguments:\n",
    "          X, np array (num_samples, num_features) - training data\n",
    "          y, np array of int (num_samples) - labels\n",
    "          batch_size, int - batch size to use\n",
    "          learning_rate, float - learning rate for gradient descent\n",
    "          reg, float - L2 regularization strength\n",
    "          epochs, int - number of epochs\n",
    "        '''\n",
    "\n",
    "        num_train = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        num_classes = np.max(y)+1\n",
    "        if self.W is None:\n",
    "            self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "\n",
    "        loss_history = np.arange(epochs).astype(np.float)\n",
    "        for epoch in range(epochs):\n",
    "            shuffled_indices = np.arange(num_train)\n",
    "            np.random.shuffle(shuffled_indices)\n",
    "            sections = np.arange(batch_size, num_train, batch_size)\n",
    "            batches_indices = np.array_split(shuffled_indices, sections)\n",
    "            \n",
    "            for i in range(sections.shape[0]): \n",
    "                batch = X[batches_indices[i],:]\n",
    "                target_index = y[batches_indices[i]]\n",
    "                loss_W, dW = linear_softmax(batch, self.W, target_index)\n",
    "                loss_l, dl = l2_regularization(self.W, reg)\n",
    "                self.W = self.W - learning_rate*(dW+dl)\n",
    "                loss = loss_W+loss_l\n",
    "                \n",
    "            loss_history[epoch] = loss\n",
    "            #print(\"Epoch %i, loss: %f\" % (epoch, loss))\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Produces classifier predictions on the set\n",
    "       \n",
    "        Arguments:\n",
    "          X, np array (test_samples, num_features)\n",
    "        Returns:\n",
    "          y_pred, np.array of int (test_samples)\n",
    "        '''\n",
    "        #y_pred = np.zeros(X.shape[0], dtype=np.int)\n",
    "        prediction = np.dot(X, self.W)\n",
    "        probs = softmax(prediction)\n",
    "        y_pred = np.array(list(map(lambda x: x.argsort()[-1], probs)))\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

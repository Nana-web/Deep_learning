{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import numpy as np\n",
    "\n",
    "from layers import FullyConnectedLayer, ReLULayer, softmax_with_cross_entropy, l2_regularization, softmax\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \"\"\" Neural network with two fully connected layers \"\"\"\n",
    "\n",
    "    def __init__(self, n_input, n_output, hidden_layer_size, reg):\n",
    "        \"\"\"\n",
    "        Initializes the neural network\n",
    "        Arguments:\n",
    "        n_input, int - dimension of the model input\n",
    "        n_output, int - number of classes to predict\n",
    "        hidden_layer_size, int - number of neurons in the hidden layer\n",
    "        reg, float - L2 regularization strength\n",
    "        \"\"\"\n",
    "        self.reg = reg\n",
    "        # TODO Create necessary layers\n",
    "        self.layer1 = FullyConnectedLayer(n_input, hidden_layer_size)\n",
    "        self.layer2 = ReLULayer()\n",
    "        self.layer3 = FullyConnectedLayer(hidden_layer_size, n_output)\n",
    "\n",
    "    def compute_loss_and_gradients(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes total loss and updates parameter gradients\n",
    "        on a batch of training examples\n",
    "        Arguments:\n",
    "        X, np array (batch_size, input_features) - input data\n",
    "        y, np array of int (batch_size) - classes\n",
    "        \"\"\"\n",
    "        # Before running forward and backward pass through the model,\n",
    "        # clear parameter gradients aggregated from the previous pass\n",
    "        # TODO Set parameter gradient to zeros\n",
    "        # Hint: using self.params() might be useful!\n",
    "        \n",
    "        params = self.params()\n",
    "        \n",
    "        for param_key in params:\n",
    "            param = params[param_key]\n",
    "            param.grad = np.zeros_like(param.grad)\n",
    "            \n",
    "        step1 = self.layer1.forward(X)\n",
    "        step2 = self.layer2.forward(step1)\n",
    "        step3 = self.layer3.forward(step2)\n",
    "        \n",
    "        loss, dL = softmax_with_cross_entropy(step3, y)\n",
    "        \n",
    "        dstep3 = self.layer3.backward(dL)\n",
    "        dstep2 = self.layer2.backward(dstep3)\n",
    "        dstep1 = self.layer1.backward(dstep2)\n",
    "        \n",
    "        # After that, implement l2 regularization on all params\n",
    "        # Hint: self.params() is useful again!\n",
    "        for param_key in params:\n",
    "            param = params[param_key]\n",
    "            loss_p, grad_p = l2_regularization(param.value, self.reg)\n",
    "            param.grad += grad_p\n",
    "            loss += loss_p\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Produces classifier predictions on the set\n",
    "        Arguments:\n",
    "          X, np array (test_samples, num_features)\n",
    "        Returns:\n",
    "          y_pred, np.array of int (test_samples)\n",
    "        \"\"\"\n",
    "        # TODO: Implement predict\n",
    "        # Hint: some of the code of the compute_loss_and_gradients\n",
    "        # can be reused\n",
    "        #pred = np.zeros(X.shape[0], np.int)\n",
    "        step1 = self.layer1.forward(X)\n",
    "        step2 = self.layer2.forward(step1)\n",
    "        step3 = self.layer3.forward(step2)\n",
    "        probs = softmax(step3)\n",
    "        pred = np.array(list(map(lambda x: x.argsort()[-1], probs)))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def params(self):\n",
    "        return {'W1': self.layer1.W, 'B1': self.layer1.B, 'W2': self.layer3.W, 'B2': self.layer3.B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

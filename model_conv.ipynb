{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_conv.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_conv.py\n",
    "import numpy as np\n",
    "\n",
    "from layers import (\n",
    "    FullyConnectedLayer, ReLULayer,\n",
    "    ConvolutionalLayer, MaxPoolingLayer, Flattener,\n",
    "    softmax_with_cross_entropy, l2_regularization, softmax\n",
    "    )\n",
    "\n",
    "\n",
    "class ConvNet:\n",
    "    \"\"\"\n",
    "    Implements a very simple conv net\n",
    "    Input -> Conv[3x3] -> Relu -> Maxpool[4x4] ->\n",
    "    Conv[3x3] -> Relu -> MaxPool[4x4] ->\n",
    "    Flatten -> FC -> Softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_output_classes, conv1_channels, conv2_channels):\n",
    "        \"\"\"\n",
    "        Initializes the neural network\n",
    "        Arguments:\n",
    "        input_shape, tuple of 3 ints - image_width, image_height, n_channels\n",
    "                                         Will be equal to (32, 32, 3)\n",
    "        n_output_classes, int - number of classes to predict\n",
    "        conv1_channels, int - number of filters in the 1st conv layer\n",
    "        conv2_channels, int - number of filters in the 2nd conv layer\n",
    "        \"\"\"\n",
    "        # TODO Create necessary layers\n",
    "        self.layer1 = ConvolutionalLayer(input_shape[2], conv1_channels, 3, 1)\n",
    "        self.layer2 = ReLULayer()\n",
    "        self.layer3 = MaxPoolingLayer(4, 4)\n",
    "        self.layer4 = ConvolutionalLayer(conv1_channels, conv2_channels, 3, 1)\n",
    "        self.layer5 = ReLULayer()\n",
    "        self.layer6 = MaxPoolingLayer(4, 4)\n",
    "        self.layer7 = Flattener()\n",
    "        self.layer8 = FullyConnectedLayer(input_shape[0]*input_shape[1]*conv2_channels//(16*16), n_output_classes)\n",
    "\n",
    "    def compute_loss_and_gradients(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes total loss and updates parameter gradients\n",
    "        on a batch of training examples\n",
    "        Arguments:\n",
    "        X, np array (batch_size, height, width, input_features) - input data\n",
    "        y, np array of int (batch_size) - classes\n",
    "        \"\"\"\n",
    "        # Before running forward and backward pass through the model,\n",
    "        # clear parameter gradients aggregated from the previous pass\n",
    "\n",
    "        # TODO Compute loss and fill param gradients\n",
    "        # Don't worry about implementing L2 regularization, we will not\n",
    "        # need it in this assignment\n",
    "        params = self.params()\n",
    "        \n",
    "        for param_key in params:\n",
    "            param = params[param_key]\n",
    "            param.grad = np.zeros_like(param.grad)\n",
    "            \n",
    "        step1 = self.layer1.forward(X)\n",
    "        step2 = self.layer2.forward(step1)\n",
    "        step3 = self.layer3.forward(step2)\n",
    "        step4 = self.layer4.forward(step3)\n",
    "        step5 = self.layer5.forward(step4)\n",
    "        step6 = self.layer6.forward(step5)\n",
    "        step7 = self.layer7.forward(step6)\n",
    "        step8 = self.layer8.forward(step7)\n",
    "        \n",
    "        loss, dL = softmax_with_cross_entropy(step8, y)\n",
    "        \n",
    "        dstep8 = self.layer8.backward(dL)\n",
    "        dstep7 = self.layer7.backward(dstep8)\n",
    "        dstep6 = self.layer6.backward(dstep7)\n",
    "        dstep5 = self.layer5.backward(dstep6)\n",
    "        dstep4 = self.layer4.backward(dstep5)\n",
    "        dstep3 = self.layer3.backward(dstep4)\n",
    "        dstep2 = self.layer2.backward(dstep3)\n",
    "        dstep1 = self.layer1.backward(dstep2)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        # You can probably copy the code from previous assignment\n",
    "        step1 = self.layer1.forward(X)\n",
    "        step2 = self.layer2.forward(step1)\n",
    "        step3 = self.layer3.forward(step2)\n",
    "        step4 = self.layer4.forward(step3)\n",
    "        step5 = self.layer5.forward(step4)\n",
    "        step6 = self.layer6.forward(step5)\n",
    "        step7 = self.layer7.forward(step6)\n",
    "        step8 = self.layer8.forward(step7)\n",
    "        probs = softmax(step8)\n",
    "        pred = np.array(list(map(lambda x: x.argsort()[-1], probs)))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def params(self):\n",
    "\n",
    "        # TODO: Aggregate all the params from all the layers\n",
    "        # which have parameters\n",
    "        return {'W1': self.layer1.W, 'B1': self.layer1.B, 'W2': self.layer4.W, 'B2': self.layer4.B, 'W3': self.layer8.W, 'B3': self.layer8.B }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
